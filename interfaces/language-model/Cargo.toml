[package]
name = "kalosm-language-model"
version = "0.3.3"
edition = "2021"
description = "A common interface for language models/transformers "
license = "MIT/Apache-2.0"
repository = "https://github.com/floneum/floneum"
authors = ["Evan Almloff"]
keywords = ["ai", "llm", "llama", "mistral", "nlp"]

[dependencies]
futures-util = "0.3.28"
llm-samplers = { workspace = true }
log = "0.4.17"
rand = "0.8.5"
serde = { version = "1.0.163", features = ["derive"], optional = true }
once_cell = "1.18.0"
tracing = "0.1.37"
candle-core.workspace = true
kalosm-sample = { workspace = true }
kalosm-common.workspace = true
kalosm-streams.workspace = true
thiserror.workspace = true
lru = { version = "0.12.3", optional = true }
safetensors = { version = "0.4.3", optional = true }
tokenizers = { workspace = true }
minijinja = { version = "2.5.0", features = ["json", "loader"] }
minijinja-contrib = { version = "2.5.0", features = ["pycompat"] }
reqwest = "0.12.12"

[dev-dependencies]
tokio = { version = "1.28.1", features = ["full"] }
kalosm = { workspace = true, features = ["language"] }
kalosm-learning = { workspace = true }
pretty_assertions = "1.4.1"
postcard = { version = "1.0.8", features = ["use-std"] }

[features]
default = ["cache"]
remote = []
serde = ["dep:serde", "safetensors"]
cache = ["serde", "dep:lru"]

[package.metadata.docs.rs]
# Features to pass to Cargo (default: [])
features = ["remote"]
